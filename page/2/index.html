<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Tramac写字的地方">
<meta property="og:type" content="website">
<meta property="og:title" content="Tramac">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Tramac">
<meta property="og:description" content="Tramac写字的地方">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Tramac">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Tramac</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Tramac" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tramac</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/17/%E7%AC%AC2%E7%AB%A0%20OpenCV%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tramac">
      <meta itemprop="description" content="Tramac写字的地方">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tramac">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/17/%E7%AC%AC2%E7%AB%A0%20OpenCV%E5%85%A5%E9%97%A8/" class="post-title-link" itemprop="url">第2章 OpenCV入门</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-17 14:18:47" itemprop="dateCreated datePublished" datetime="2020-07-17T14:18:47+08:00">2020-07-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="OpenCV的结构和内容"><a href="#OpenCV的结构和内容" class="headerlink" title="OpenCV的结构和内容"></a>OpenCV的结构和内容</h4><p>OpenCV主体分为五个模块：</p>
<ul>
<li>CV模块：包含基本的图像处理函数和高级的计算机视觉算法。</li>
<li>ML库：机器学习库，包含一些基于统计的分类和聚类工具。</li>
<li>HighGUI：包含图像和视频输入/输出的函数。</li>
<li>CXCore：包含OpenCV的一些基本数据结构和相关函数。</li>
<li>CvAux模块：存放一些即将被淘汰的算法和函数。</li>
</ul>
<h4 id="显示图像"><a href="#显示图像" class="headerlink" title="显示图像"></a>显示图像</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;opencv&#x2F;cv.h&quot;</span><br><span class="line">#include &quot;opencv&#x2F;highgui.h&quot;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    IplImage *img &#x3D; cvLoadImage(&quot;.&#x2F;Lena.jpg&quot;);</span><br><span class="line">    cvNamedWindow(&quot;Example1&quot;, CV_WINDOW_AUTOSIZE);</span><br><span class="line">    cvShowImage(&quot;Example1&quot;, img);</span><br><span class="line">    cvWaitKey(0);</span><br><span class="line">    cvReleaseImage(&amp;img);</span><br><span class="line">    cvDestroyWindow(&quot;Example1&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>cvLoadImage()</code>函数可读取绝大多数格式类型的图像文件，包括BMP,DIB,JPEG,JPE,PNG,PBM,TIFF等。该函数执行后返回一个指针，此指针指向一块为描述该图像文件的数据结构（IplImage）而分配的内存块。</p>
<h4 id="简单的变换"><a href="#简单的变换" class="headerlink" title="简单的变换"></a>简单的变换</h4><p>实现变换的方式：新建一个IplImage结构，然后对原图执行某种操作，将结构写入到新建的结构中，返回。<br>新建IplImage的方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">IplImage* out &#x3D; cvCreateImage(</span><br><span class="line">    cvSize(in-&gt;width &#x2F; 2, in-&gt;height &#x2F; 2),</span><br><span class="line">    in-&gt;depth,</span><br><span class="line">    in-&gt;nChannels);</span><br></pre></td></tr></table></figure>



<p>分配新的图像空间时是从旧的图像中读取所需的信息。在OpenCV中，所有的重要的数据都是以数据体的形式实现，并且以数据体指针的形式传递。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/17/%E5%AD%A6%E4%B9%A0OpenCV%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tramac">
      <meta itemprop="description" content="Tramac写字的地方">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tramac">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/17/%E5%AD%A6%E4%B9%A0OpenCV%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/" class="post-title-link" itemprop="url">学习OpenCV环境准备</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-17 14:17:51" itemprop="dateCreated datePublished" datetime="2020-07-17T14:17:51+08:00">2020-07-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="OpenCV配置"><a href="#OpenCV配置" class="headerlink" title="OpenCV配置"></a>OpenCV配置</h4><p>由于我的Ubuntu 16.04不知道什么时候已经安装了OpenCV，所以这里省略了安装过程。注意的是我的OpenCV应该不是按照官网的编译方式安装的，应该是之前有一些环境配置时所需依赖时以其他的方式安装的。</p>
<ul>
<li>使用vs code编译OpenCV程序</li>
</ul>
<p>在vs code编译器中，用简单的读取图片的代码测试，运行时出现下面错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;tmp&#x2F;cc9zYXpi.o: In function &#39;main&#39;:</span><br><span class="line">&#x2F;home&#x2F;xxx&#x2F;c++&#x2F;main.cpp:5: undefined reference to &#39;cvLoadImage&#39;</span><br><span class="line">&#x2F;home&#x2F;xxx&#x2F;c++&#x2F;main.cpp:6: undefined reference to &#39;cvNamedWindow&#39;</span><br><span class="line">&#x2F;home&#x2F;xxx&#x2F;c++&#x2F;main.cpp:7: undefined reference to &#39;cvShowImage&#39;</span><br><span class="line">&#x2F;home&#x2F;xxx&#x2F;c++&#x2F;main.cpp:8: undefined reference to &#39;cvWaitKey&#39;</span><br><span class="line">&#x2F;home&#x2F;xxx&#x2F;c++&#x2F;main.cpp:9: undefined reference to &#39;cvReleaseImage&#39;</span><br><span class="line">&#x2F;home&#x2F;xxx&#x2F;c++&#x2F;main.cpp:10: undefined reference to &#39;cvDestroyWindow&#39;</span><br><span class="line">collect2: error: ld returned 1 exit status</span><br><span class="line">The terminal process terminated with exit code: 1</span><br></pre></td></tr></table></figure>



<p>我对于该错误出现的推测是，因为测试程序是在之前配置运行c++的环境下运行的，所以设置的默认编译指令也是用的之前的，所以对于当前的并不适用，应该需要对于OpenCV所需的指令重新设定默认编译指令。</p>
<ul>
<li>一种在终端编译的方式</li>
</ul>
<p>在vs code中由于设置的问题出现报错，那有一种通用的方式是在终端输入命令行完成编译。命令行如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g++ main.cpp -o main_out &#96;pkg-config --cflags --libs opencv&#96;</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：这里是`而不是‘，否则无法编译成功。</p>
<p><code>pkg-config</code>可用于列举某个库的相关信息，比如此库的路径/相关头文件等。比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~$ pkg-config --cflags opencv</span><br><span class="line">-I&#x2F;usr&#x2F;include&#x2F;opencv</span><br><span class="line"></span><br><span class="line">~$ pkg-config --libs opencv</span><br><span class="line">&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_calib3d.so -lopencv_calib3d &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_contrib.so -lopencv_contrib &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_core.so -lopencv_core &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_features2d.so -lopencv_features2d &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_flann.so -lopencv_flann &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_gpu.so -lopencv_gpu &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_highgui.so -lopencv_highgui &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_imgproc.so -lopencv_imgproc &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_legacy.so -lopencv_legacy &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_ml.so -lopencv_ml &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_objdetect.so -lopencv_objdetect &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_ocl.so -lopencv_ocl &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_photo.so -lopencv_photo &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_stitching.so -lopencv_stitching &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_superres.so -lopencv_superres &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_ts.so -lopencv_ts &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_video.so -lopencv_video &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_videostab.so -lopencv_videostab</span><br></pre></td></tr></table></figure>



<p>通过这种方式就可以开始OpenCV的学习了，但是这样会很不方便吧，所以还是要在vs code中配置一套属于OpenCV的环境。</p>
<ul>
<li>配置vs code编译OpenCV程序的环境</li>
</ul>
<p>由上面的过程可以知道，究其原因就是之前的vs code设置的默认指令不匹配造成。同时也知道正确的指令应该是<code>g++ main.cpp -o main_test.out &#39;pkg-config --cflags --libs opencv&#39;</code>。<br>配置vs code的环境大同小异，这里简述一下：</p>
<blockquote>
<p>1.新建文件夹LearningOpenCV<br>2.在根目录下新建.vscode文件夹，并在该文件夹下新建launch.json,settings.json,tasks.json三个设置文件<br>3.其中launch.json,settings.json的设置可以参考其他，重要的是tasks.json中args参数的设置，这个是默认指令的设置。我的tasks.json设置如下：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; See https:&#x2F;&#x2F;go.microsoft.com&#x2F;fwlink&#x2F;?LinkId&#x3D;733558</span><br><span class="line">    &#x2F;&#x2F; for the documentation about the tasks.json format</span><br><span class="line">    &quot;version&quot;: &quot;2.0.0&quot;,</span><br><span class="line">    &quot;tasks&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;label&quot;: &quot;build&quot;,</span><br><span class="line">            &quot;type&quot;: &quot;shell&quot;,</span><br><span class="line">            &quot;command&quot;: &quot;g++&quot;,</span><br><span class="line">            &quot;args&quot;:[&quot;-g&quot;,&quot;$&#123;workspaceRoot&#125;&#x2F;main.cpp&quot;,&quot;-o&quot;,&quot;main_test.out&quot;,&quot;&#96;pkg-config&quot;,&quot;--cflags&quot;,&quot;--libs&quot;,&quot;opencv&#96;&quot;],</span><br><span class="line">            &quot;presentation&quot;: &#123;</span><br><span class="line">                &quot;reveal&quot;: &quot;always&quot;,</span><br><span class="line">                &quot;panel&quot;: &quot;new&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;problemMatcher&quot;:</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;owner&quot;: &quot;cpp&quot;,</span><br><span class="line">                &quot;fileLocation&quot;:[&quot;relative&quot;, &quot;$&#123;workspaceRoot&#125;&quot;],</span><br><span class="line">                &quot;pattern&quot;:[</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;regexp&quot;: &quot;^([^\\\\s].*)\\\\((\\\\d+,\\\\d+)\\\\):\\\\s*(.*)$&quot;,</span><br><span class="line">                        &quot;file&quot;: 1,</span><br><span class="line">                        &quot;line&quot;: 2,</span><br><span class="line">                        &quot;column&quot;: 3,</span><br><span class="line">                        &quot;severity&quot;: 4,</span><br><span class="line">                        &quot;location&quot;: 2,</span><br><span class="line">                        &quot;message&quot;: 5</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到tasks.json中的args参数正是我们上面说到的终端指令。<br><strong>注意</strong>：由于args的参数设置时不能包含空格，所以<code>pkg-config --cflags --libs opencv</code>需要不同的引号分开，同时`注意不要搞错或者忘记。<br>到此为止，vs code的OpenCV编译环境已经完成了，接下来就在Ubuntu下面开始OpenCV的学习吧！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/17/Finetuning%20with%20Tensorflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tramac">
      <meta itemprop="description" content="Tramac写字的地方">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tramac">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/17/Finetuning%20with%20Tensorflow/" class="post-title-link" itemprop="url">Finetuning with Tensorflow</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-17 14:16:08" itemprop="dateCreated datePublished" datetime="2020-07-17T14:16:08+08:00">2020-07-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>很多网络的特征提取部分都会用到fine-tunning,比如resnet-50,inception等,该文章以AlexNet为例,分析tensorflow如何进行微调</p>
<p><strong>finetuning的三要素:</strong></p>
<ul>
<li>预训练模型,如resnet_v2_50.npy、resnet_v2_50.ckpt等。</li>
<li>模型的网络结构定义。</li>
<li>所需从预训练模型中恢复的变量，通常以排除的方式给出。</li>
</ul>
<p><strong>Tips:</strong></p>
<ul>
<li><p>所定义网络结构中的变量名需要和预训练模型中的变量名保持相同。预训练模型中的变量名可有以下方式查看:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 使用tf.train.NewCheckpointReader直接读取ckpt文件里的变量</span><br><span class="line">from tensorflow.python import pywrap_tensorflow</span><br><span class="line"></span><br><span class="line">reader &#x3D; pywarp_tensorflow.NewCheckpointReader(checkpoint_path)</span><br><span class="line">var_to_shape_map &#x3D; reader.get_variable_to_shape_map()</span><br><span class="line">for key in var_to_shape_map:</span><br><span class="line">    print(&quot;tensor_name: &quot;, key)</span><br></pre></td></tr></table></figure>
</li>
<li><p>恢复部分变量时可借助slim。如:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">variables_to_restore &#x3D; slim.get_varibales_to_restore(exclude&#x3D;[args.resnet_model] + &quot;logits&quot;, &quot;optimizer_vars&quot;,  &quot;DeepLab_v3&#x2F;ASPP_layer&quot;, &quot;DeepLab_v3&#x2F;logits&quot;])</span><br><span class="line">restorer &#x3D; tf.train.Saver(variables_to_restore)</span><br><span class="line">restorer.restore(sess, &quot;.&#x2F;resnet&#x2F;checkpoints&#x2F;&quot; + args.resnet_model + &quot;.&#x2F;ckpt&quot;)</span><br></pre></td></tr></table></figure>

</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/17/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tramac">
      <meta itemprop="description" content="Tramac写字的地方">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tramac">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/17/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-PyTorch/" class="post-title-link" itemprop="url">分布式训练-PyTorch</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-17 13:13:21" itemprop="dateCreated datePublished" datetime="2020-07-17T13:13:21+08:00">2020-07-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>并行训练（数据并行与模型并行）与分布式训练是深度学习中加速训练的两种常用方式，相对于并行训练，分布式是更优的加速方案，也是PyTorch官方推荐的方法：<br>Multi-Process Single-GPU<br>This is the highly recommended way to use DistributedDataParallel, with multiple processes, each of which operates on a single GPU. This is currently the fastest approach to do data parallel training using PyTorch and applies to both single-node(multi-GPU) and multi-node data parallel training. It is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training.</p>
<p>个人理解：分布式训练其实也是并行训练的一种方式，只是相对于数据并行、模型并行有所不同。简单来说，分布式针对多机多卡，而数据并行针对单机多卡。</p>
<h3 id="Distributed-Training-Code"><a href="#Distributed-Training-Code" class="headerlink" title="Distributed Training Code"></a>Distributed Training Code</h3><p>下面内容主要指出分布式训练代码中与常规训练过程之间的主要区别。</p>
<h4 id="Imports"><a href="#Imports" class="headerlink" title="Imports"></a>Imports</h4><p>分布式训练主要涉及到的库主要有<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel" target="_blank" rel="noopener">torch.nn.parallel</a>，<a href="https://pytorch.org/docs/stable/distributed.html" target="_blank" rel="noopener">torch.distributed</a>，<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler" target="_blank" rel="noopener">torch.utils.data.distributed</a>以及<a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener">torch.multiprocessing</a>。需要注意的是我们需要把multiprocessing的start method设置为spawn或forkserver（仅Python3支持）,因为默认的方法为fork，容易导致死锁情况发生。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import sys</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    torch.multiprocessing.set_start_method(&#39;spawn&#39;)</span><br><span class="line"></span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.parallel</span><br><span class="line">import torch.distributed as dist</span><br><span class="line">import torch.optim</span><br><span class="line">import torch.utils.data</span><br><span class="line">import torch.utils.data.distributed</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">import torchvision.datasets as datasets</span><br><span class="line">import torchvision.models as models</span><br></pre></td></tr></table></figure>

<h4 id="Train-Function"><a href="#Train-Function" class="headerlink" title="Train Function"></a>Train Function</h4><p>第一个区别，在分布式训练过程中，需要设置数据的<code>non_blocking</code>的属性设置为<code>True</code>。该操作使得不同GPU上的数据副本允许重叠计算，并且可以输出训练时的统计数据，以便我们可以跟踪整个训练过程的进度。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def train(train_loader, model, criterion, optimizer, epoch):</span><br><span class="line">    # switch to train mode</span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    for i, (input, target) in enumerate(train_loader):</span><br><span class="line">        # Create non_blocking tensors for distributed training</span><br><span class="line">        input &#x3D; input.cuda(non_blocking&#x3D;True)</span><br><span class="line">        target &#x3D; target.cuda(non_blocking&#x3D;True)</span><br><span class="line">        </span><br><span class="line">        # compute output</span><br><span class="line">        output &#x3D; model(input)</span><br><span class="line">        loss &#x3D; criterion(output, target)</span><br><span class="line">        </span><br><span class="line">        # compute gradients in a backward pass</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        </span><br><span class="line">        # Call step of optimizer to update model params</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        if i % 10 &#x3D;&#x3D; 0:</span><br><span class="line">            print(&#39;Epoch: [&#123;0&#125;][&#123;1&#125;&#x2F;&#123;2&#125;]\t&#39;</span><br><span class="line">                  &#39;Loss &#123;loss.val:.4f&#125; (&#123;loss.avg:.4f&#125;)&#39;.format(</span><br><span class="line">                  epoch, i, len(train_loader), loss&#x3D;losses))</span><br><span class="line"></span><br><span class="line">def adjust_learning_rate(initial_lr, optimizer, epoch):</span><br><span class="line">    &quot;&quot;&quot;Sets the learning rate to the initial LR decayed by 10 every 30 epochs&quot;&quot;&quot;</span><br><span class="line">    lr &#x3D; initial_lr * (0.1 ** (epoch &#x2F;&#x2F; 30))</span><br><span class="line">    for param_group in optimizer.param_groups:</span><br><span class="line">        param_group[&#39;lr&#39;] &#x3D; lr</span><br></pre></td></tr></table></figure>

<h4 id="Validation-Function"><a href="#Validation-Function" class="headerlink" title="Validation Function"></a>Validation Function</h4><p>与训练过程相同，唯一的区别是获取数据时需要设置<code>non_blocking=True</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def validation(val_loader, model, criterion):</span><br><span class="line">    # switch to evaluate mode</span><br><span class="line">    model.eval()</span><br><span class="line">    </span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for i, (input, target) in enumerate(val_loader):</span><br><span class="line">            input &#x3D; input.cuda(non_blocking&#x3D;True)</span><br><span class="line">            target &#x3D; target.cuda(non_blocking&#x3D;True)</span><br><span class="line">            </span><br><span class="line">            # compute output</span><br><span class="line">            output &#x3D; model(input)</span><br><span class="line">            loss &#x3D; criterion(output, target)</span><br><span class="line">            </span><br><span class="line">            if i % 100 &#x3D;&#x3D; 0:</span><br><span class="line">                print(&#39;Test: [&#123;0&#125;&#x2F;&#123;1&#125;]\t&#39;</span><br><span class="line">                      &#39;Loss &#123;loss.val:.4f&#125; (&#123;loss.avg:.4f&#125;))&#39;.format(</span><br><span class="line">                       i, len(val_loader), loss&#x3D;losses))</span><br></pre></td></tr></table></figure>

<h4 id="Inputs"><a href="#Inputs" class="headerlink" title="Inputs"></a>Inputs</h4><p>相对于标准模型训练，分布式训练在定义数据输入时也略有不同，有些参数为分布式训练任务特定的。参数说明如下：</p>
<ul>
<li><strong>batch_size</strong>-batch size for <em>each</em> process in the distributed training group. Total batch size across distributed model is batch_size*world_size</li>
<li><strong>workers</strong> - number of worker processes used with the dataloaders in each process</li>
<li><strong>num_epochs</strong> - total number of epochs to train for</li>
<li><strong>starting_lr</strong> - starting learning rate for training</li>
<li><strong>world_size</strong> - number of processes in the distributed training environment</li>
<li><strong>dist_backend</strong> - backend to use for distributed training communication (i.e. NCCL, Gloo, MPI, etc.). In this tutorial, since we are using several multi-gpu nodes, NCCL is suggested.</li>
<li><strong>dist_url</strong> - URL to specify the initialization method of the process group. This may contain the IP address and port of the rank0 process or be a non-existant file on a shared file system. Here, since we do not have a shared file system this will incorporate the <strong>node0-privateIP</strong> and the port on node0 to use.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Collect Inputs...&quot;)</span><br><span class="line"></span><br><span class="line"># Batch Size for training and testing</span><br><span class="line">batch_size &#x3D; 32</span><br><span class="line"></span><br><span class="line"># Number of additional worker processes for dataloading</span><br><span class="line">workers &#x3D; 2</span><br><span class="line"></span><br><span class="line"># Number of epochs to train for</span><br><span class="line">num_epochs &#x3D; 2</span><br><span class="line"></span><br><span class="line"># Starting Learning Rate</span><br><span class="line">starting_lr &#x3D; 0.1</span><br><span class="line"></span><br><span class="line"># Number of distributed processes</span><br><span class="line">world_size &#x3D; 4</span><br><span class="line"></span><br><span class="line"># Distributed backend type</span><br><span class="line">dist_backend &#x3D; &#39;nccl&#39;</span><br><span class="line"></span><br><span class="line"># Url used to setup distributed training</span><br><span class="line">dist_url &#x3D; &quot;tcp:&#x2F;&#x2F;172.31.22.234:23456&quot;</span><br></pre></td></tr></table></figure>

<h4 id="Initialize-process-group"><a href="#Initialize-process-group" class="headerlink" title="Initialize process group"></a>Initialize process group</h4><ol>
<li><p>设置进程组。该过程可由函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.init_process_group</span><br></pre></td></tr></table></figure>

<p>实现。函数的参数说明如下：</p>
<ul>
<li><strong>backend</strong>-the backend to use (i.e. NCCL, Gloo, MPI, etc.)</li>
<li><strong>init_method</strong>-which is either a url containing the address and port of the rank0 machine or a path to a non-existant file on the shared file system. Note, to use the file init_method, all machines must have access to the file, similarly for the url method, all machines must be able to communicate on the network so make sure to configure any firewalls and network settings to accomodate.</li>
<li><strong>rank</strong>-the rank of this process when run</li>
<li><strong>world_size</strong>-the number of processes in the collective<br>The <em>init_method</em> input can also be “env://”. In this case, the address and port of the rank0 machine will be read from the following two environment variables respectively: MASTER_ADDR, MASTER_PORT. If <em>rank*and *world_size</em> arguments are not specified in the <em>init_process_group</em> function, they both can be read from the following two environment variables respectively as well: RANK, WORLD_SIZE.</li>
</ul>
</li>
<li><p>设置进程的lock_rank。该操作用于为进程指定设备（即使用哪个GPU），同时也用于创建分布式数据并行模型时指定设备。</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Initialize Process Group...&quot;)</span><br><span class="line"># Initialize Process Group</span><br><span class="line"># v1 - init with url</span><br><span class="line">dist.init_process_group(backend&#x3D;dist_backend, init_method&#x3D;dist_url, rank&#x3D;int(sys.argv[1]), world_size&#x3D;world_size)</span><br><span class="line"># v2 - init with file</span><br><span class="line"># dist.init_process_group(backend&#x3D;&quot;nccl&quot;, init_method&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;home&#x2F;ubuntu&#x2F;pt-distributed-tutorial&#x2F;trainfile&quot;, rank&#x3D;int(sys.argv[1]), world_size&#x3D;world_size)</span><br><span class="line"># v3 - init with environment variables</span><br><span class="line"># dist.init_process_group(backend&#x3D;&quot;nccl&quot;, init_method&#x3D;&quot;env:&#x2F;&#x2F;&quot;, rank&#x3D;int(sys.argv[1]), world_size&#x3D;world_size)</span><br><span class="line"></span><br><span class="line"># Establish Local Rank and set device on this node</span><br><span class="line">local_rank &#x3D; int(sys.argv[2])</span><br><span class="line">dp_device_ids &#x3D; [local_rank]</span><br><span class="line">torch.cuda.set_device(local_rank)</span><br></pre></td></tr></table></figure>

<h4 id="Initialize-Model"><a href="#Initialize-Model" class="headerlink" title="Initialize Model"></a>Initialize Model</h4><p>在定义模型时，需要将其指定为分布式模式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Initialize Model...&quot;)</span><br><span class="line"># Construct Model</span><br><span class="line">model &#x3D; models.resnet18(pretrained&#x3D;False).cuda()</span><br><span class="line"># Make model DistributedDataParallel</span><br><span class="line">model &#x3D; torch.nn.parallel.DistributedDataParallel(model, device_ids&#x3D;dp_device_ids, output_device&#x3D;local_rank)</span><br><span class="line"></span><br><span class="line"># define loss function (criterion) and optimizer</span><br><span class="line">criterion &#x3D; nn.CrossEntropyLoss().cuda()</span><br><span class="line">optimizer &#x3D; torch.optim.SGD(model.parameters(), starting_lr, momentum&#x3D;0.9, weight_decay&#x3D;1e-4)</span><br></pre></td></tr></table></figure>

<h4 id="Initialize-Dataloaders"><a href="#Initialize-Dataloaders" class="headerlink" title="Initialize Dataloaders"></a>Initialize Dataloaders</h4><p>分布式训练的最后一个特定情况是将训练数据指定为DistributedSampler，与DistributedDataParallel模型结合使用。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Initialize Dataloaders...&quot;)</span><br><span class="line">transform &#x3D; transforms.Compose(</span><br><span class="line">    [transforms.Resize(224),</span><br><span class="line">     transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</span><br><span class="line"></span><br><span class="line"># Initialize Datasets. STL10 will automatically download if not present</span><br><span class="line">trainset &#x3D; datasets.STL10(root&#x3D;&#39;.&#x2F;data&#39;, split&#x3D;&#39;train&#39;, download&#x3D;True, transform&#x3D;transform)</span><br><span class="line">valset &#x3D; datasets.STL10(root&#x3D;&#39;.&#x2F;data&#39;, split&#x3D;&#39;test&#39;, download&#x3D;True, transform&#x3D;transform)</span><br><span class="line"></span><br><span class="line"># Create DistributedSampler to handle distributing the dataset across nodes when training</span><br><span class="line"># This can only be called after torch.distributed.init_process_group is called</span><br><span class="line">train_sampler &#x3D; torch.utils.data.distributed.DistributedSampler(trainset)</span><br><span class="line"></span><br><span class="line"># Create the Dataloaders to feed data to the training and validation steps</span><br><span class="line">train_loader &#x3D; torch.utils.data.DataLoader(trainset, batch_size&#x3D;batch_size, shuffle&#x3D;(train_sampler is None), num_workers&#x3D;workers, pin_memory&#x3D;False, sampler&#x3D;train_sampler)</span><br><span class="line">val_loader &#x3D; torch.utils.data.DataLoader(valset, batch_size&#x3D;batch_size, shuffle&#x3D;False, num_workers&#x3D;workers, pin_memory&#x3D;False)</span><br></pre></td></tr></table></figure>

<h4 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h4><p>开始训练，与标准模型训练唯一的不同的需要更新DistributedSampler的epoch。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    # Set epoch count for DistributedSampler</span><br><span class="line">    train_sampler.set_epoch(epoch)</span><br><span class="line">    </span><br><span class="line">    # Adjust learning rate according to schedule</span><br><span class="line">    adjust_learning_rate(starting_lr, optimizer, epoch)</span><br><span class="line">    </span><br><span class="line">    # train for one epoch</span><br><span class="line">    print(&quot;\nBegin Training Epoch &#123;&#125;&quot;.format(epoch+1))</span><br><span class="line">    train(train_loader, model, criterion, optimizer, epoch)</span><br><span class="line">    </span><br><span class="line">    # evaluate on validation set</span><br><span class="line">    print(&quot;Begin Validation @ Epoch &#123;&#125;&quot;.format(epoch+1))</span><br><span class="line">    validate(val_loader, model, criterion)</span><br></pre></td></tr></table></figure>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>PyTorch中，分布式训练相对于标准训练主要有以下几点不同：</p>
<ul>
<li>更改输入数据的<code>non_blocking</code>属性。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input &#x3D; input.cuda(non_blocking&#x3D;True)</span><br><span class="line">target &#x3D; target.cuda(non_blocking&#x3D;True)</span><br></pre></td></tr></table></figure>

<ul>
<li>初始化进程组，设置local rank</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dist.init_process_group(backend&#x3D;dist_backend, init_method&#x3D;dist_url, rank&#x3D;int(sys.argv[1]), world_size&#x3D;world_size)</span><br><span class="line"></span><br><span class="line"># Establish Local Rank and set device on this node</span><br><span class="line">local_rank &#x3D; int(sys.argv[2])</span><br><span class="line">dp_device_ids &#x3D; [local_rank]</span><br><span class="line">torch.cuda.set_device(local_rank)</span><br></pre></td></tr></table></figure>

<ul>
<li>指定模型为分布式数据并行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; torch.nn.parallel.DistributedDataParallel(model, device_ids&#x3D;dp_device_ids, output_device&#x3D;local_rank)</span><br></pre></td></tr></table></figure>

<ul>
<li>指定数据集为分布式样本</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_sampler &#x3D; torch.utils.data.distributed.DistributedSampler(trainset)</span><br></pre></td></tr></table></figure>

<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h3><ul>
<li><a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html#writing-distributed-applications-with-pytorch" target="_blank" rel="noopener">PYTORCH 1.0 DISTRIBUTED TRAINER WITH AMAZON AWS</a></li>
<li><a href="https://github.com/seba-1511/dist_tuto.pth" target="_blank" rel="noopener">PyTorch Distributed Tutorial</a></li>
<li><a href="https://github.com/pytorch/examples/tree/master/imagenet" target="_blank" rel="noopener">PyTorch ImageNet Example</a></li>
<li><a href="https://blog.csdn.net/zwqjoy/article/details/89415933" target="_blank" rel="noopener">Pytorch 1.0 分布式训练初探</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/17/Paper-reading%20list/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tramac">
      <meta itemprop="description" content="Tramac写字的地方">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tramac">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/17/Paper-reading%20list/" class="post-title-link" itemprop="url">Paper-reading list</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-17 10:34:18" itemprop="dateCreated datePublished" datetime="2020-07-17T10:34:18+08:00">2020-07-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h2><h4 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a><a href="https://github.com/Tramac/Awesome-semantic-segmentation-pytorch" target="_blank" rel="noopener">GitHub</a></h4><p>1.[<strong>AdaptSegNet</strong>] Learning to Adapt Structured Output Space for Semantic Segmentation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1802.10349.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/wasidennis/AdaptSegNet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>2.[<strong>DAM/DCM</strong>] Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical Image Segmentations with Adversarial Loss-IJCAI2018&lt;<a href="https://arxiv.org/pdf/1804.10916.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>3.[<strong>FCAN</strong>] Fully Convolutional Adaptation Networks for Semantic Segmentation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1804.08286.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>4.[<strong>DenseASPP</strong>] DenseASPP for Semantic Segmentation in Street Scenes-CVPR2018&lt;<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/DeepMotionAIResearch/DenseASPP" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>5.Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation-CVPR2018&lt;<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>6.[<strong>AotofocusLayer</strong>] Autofocus Layer for Semantic Segmentation-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1805.08403.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yaq007/Autofocus-Layer" target="_blank" rel="noopener">Code-Pytorch</a>&gt;<br>7.[<strong>PDV-Net</strong>] Automatic Segmentation of Pulmonary Lobes Using a Progressive Dense V-Network-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00889-5_32.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>8.[<strong>RR-SegSE</strong>] Adaptive feature recombination and recalibration for semantic segmentation: application to brain tumor segmentation in MRI-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1806.02318.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/sergiormpereira/rr_segse" target="_blank" rel="noopener">Code</a>&gt;<br>9.[<strong>HD-Net</strong>] Fine-Grained Segmentation Using Hierarchical Dilated Neural Networks-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00937-3_56.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>10.[<strong>U-JAPA-Net</strong>] 3D U-JAPA-Net: Mixture of Convolutional Networks for Abdominal Multi-organ CT Segmentation-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00937-3_49.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>11.[<strong>CompNet</strong>] CompNet: Complementary Segmentation Network for Brain MRI Extraction-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1804.00521.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/raun1/MICCAI2018---Complementary_Segmentation_Network-Raw-Code" target="_blank" rel="noopener">Code-Keras</a>&gt;<br>12.Deep Learning-Based Boundary Detection for Model-Based Segmentation with Application to MR Prostate Segmentation-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00937-3_59.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>13.[<strong>RS-Net</strong>] RS-Net: Regression-Segmentation 3D CNN for Synthesis of Full Resolution Missing Brain MRI in the Presence of Tumours-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1807.10972v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/RagMeh11/RS-Net" target="_blank" rel="noopener">Code</a>&gt;<br>14.CT-Realistic Lung Nodule Simulation from 3D Conditional Generative Adversarial Networks for Robust Lung Segmentation-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1806.04051.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>15.[<strong>CB-GANs</strong>] Learning Data Augmentation for Brain Tumor Segmentation with Coarse-to-Fine Generative Adversarial Networks-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1805.11291.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>16.[<strong>FSENet</strong>] Focus, Segment and Erase: An Efficient Network for Multi-Label Brain Tumor Segmentation-ECCV2018&lt;<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xuan_Chen_Focus_Segment_and_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/LaviniaChen/Segment-and-Erase-Network" target="_blank" rel="noopener">Code-Pytorch</a>&gt;<br>17.[<strong>DeepLabv3+</strong>] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation-ECCV2018&lt;<a href="https://arxiv.org/pdf/1802.02611v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>18.[<strong>ExFuse</strong>] ExFuse: Enhancing Feature Fusion for Semantic Segmentation-ECCV2018&lt;<a href="https://arxiv.org/pdf/1804.03821.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>19.[<strong>ESPNet</strong>] ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation-ECCV2018&lt;<a href="https://arxiv.org/pdf/1803.06815v2.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/sacmehta/ESPNet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>20.[<strong>EncNet</strong>] Context Encoding for Semantic Segmentation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1803.08904v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/zhanghang1989/PyTorch-Encoding" target="_blank" rel="noopener">Code-Pytorch</a>&gt;<br>21.[<strong>PSPNet</strong>] Pyramid Scene Parsing Network-CVPR2017&lt;<a href="https://arxiv.org/pdf/1612.01105.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/hszhao/PSPNet" target="_blank" rel="noopener">Code-Caffe</a>&gt;<br>22.[<strong>DANet</strong>] Dual Attention Network for Scene Segmentation-CVPR2019&lt;<a href="https://arxiv.org/pdf/1809.02983.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/junfu1115/DANet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>23.[<strong>BiSeNet</strong>] BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation-ECCV-2018&lt;<a href="https://arxiv.org/pdf/1808.00897.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ycszen/TorchSeg" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>24.[<strong>Fast-SCNN</strong>] Fast-SCNN: Fast Semantic Segmentation Network-2019&lt;<a href="https://arxiv.org/pdf/1902.04502.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Tramac/Fast-SCNN-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>25.[<strong>ICNet</strong>] ICNet for Real-Time Semantic Segmentation on High-Resolution Images-ECCV2018&lt;<a href="https://arxiv.org/pdf/1704.08545.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Tramac/awesome-semantic-segmentation-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://github.com/hszhao/ICNet" target="_blank" rel="noopener">Code-Caffe</a>&gt;<br>26.[<strong>DUNet</strong>] Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation-CVPR2019&lt;<a href="https://arxiv.org/abs/1903.02120.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Tramac/awesome-semantic-segmentation-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>27.[<strong>ENet</strong>] ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation-2016&lt;<a href="https://arxiv.org/abs/1606.02147.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/TimoSaemann/ENet" target="_blank" rel="noopener">Code-Caffe</a>&gt;&lt;<a href="https://github.com/davidtvs/PyTorch-ENet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://github.com/kwotsin/TensorFlow-ENet" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>28.[<strong>CCNet</strong>] CCNet: Criss-Cross Attention for Semantic Segmentation-2018&lt;<a href="https://arxiv.org/abs/1811.11721v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/speedinghzl/CCNet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>29.[<strong>OCNet</strong>] OCNet: Object Context Network for Scene Parsing-2018&lt;<a href="https://arxiv.org/abs/1809.00916.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/PkuRainBow/OCNet.pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>30.[<strong>HRNet</strong>] High-Resolution Representations for Labeling Pixels and Regions-2019&lt;<a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/HRNet/HRNet-Semantic-Segmentation" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</p>
<h2 id="Panoptic-Segmentation"><a href="#Panoptic-Segmentation" class="headerlink" title="Panoptic Segmentation"></a>Panoptic Segmentation</h2><p>1.[<strong>Panoptic FPN</strong>] Panoptic Feature Pyramid Networks-Arxiv2019&lt;<a href="https://arxiv.org/pdf/1901.02446.pdf" target="_blank" rel="noopener">Paper</a>&gt;</p>
<h2 id="Super-Resolution"><a href="#Super-Resolution" class="headerlink" title="Super-Resolution"></a>Super-Resolution</h2><p>1.[<strong>mDCSRN</strong>] Efficient and Accurate MRI Super-Resolution using a Generative Adversarial Network and 3D Multi-Level Densely Connected Network-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1803.01417.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>2.[<strong>RDN</strong>] Residual Dense Network for Image Super-Resolution-CVPR2018&lt;<a href="https://arxiv.org/pdf/1802.08797.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yulunzhang/RDN" target="_blank" rel="noopener">Code-Torch</a>&gt;&lt;<a href="https://github.com/thstkdgus35/EDSR-PyTorch" target="_blank" rel="noopener">Code-Pytorch</a>&gt;&lt;<a href="https://github.com/hengchuan/RDN-TensorFlow" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>1.[<strong>FocalLoss</strong>]Focal Loss for Dense Object Detection-ICCV2017&lt;<a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/facebookresearch/Detectron" target="_blank" rel="noopener">Code-Caffe2</a>&gt;</p>
<h2 id="Networks-Architecture"><a href="#Networks-Architecture" class="headerlink" title="Networks Architecture"></a>Networks Architecture</h2><p>1.[<strong>DLA</strong>] Deep Layer Aggregation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1707.06484.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ucbdrive/dla" target="_blank" rel="noopener">Code-Pytorch</a>&gt;<br>2.[<strong>DualSkipNet</strong>] Dual Skipping Networks-CVPR2018&lt;<a href="https://arxiv.org/pdf/1710.10386.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>3.[<strong>SkipNet</strong>] SkipNet: Learning Dynamic Routing in Convolutional Networks-ECCV2018&lt;<a href="https://arxiv.org/pdf/1711.09485.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ucbdrive/skipnet" target="_blank" rel="noopener">Code-Pytorch</a>&gt;<br>4.[<strong>DRN</strong>] Dilated Residual Networks-CVPR2017&lt;<a href="https://arxiv.org/pdf/1705.09914.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/fyu/drn" target="_blank" rel="noopener">Code-Pytorch</a>&gt;<br>5.[<strong>CapsNet</strong>] Dynamic Routing Between Capsules-NIPS2017&lt;<a href="https://arxiv.org/pdf/1710.09829.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/naturomics/CapsNet-Tensorflow" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>6.[<strong>BlockQNN</strong>] Practical Block-wise Neural Network Architecture Generation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1708.05552.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>7.[<strong>MobileNetV2</strong>] MobileNetV2: Inverted Residuals and Linear Bottlenecks-CVPR2018&lt;<a href="https://128.84.21.199/pdf/1801.04381.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>8.[<strong>Non-Local</strong>] Non-local Neural Networks-CVPR2018&lt;<a href="https://arxiv.org/pdf/1711.07971.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/facebookresearch/video-nonlocal-net" target="_blank" rel="noopener">Code</a>&gt;</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Tramac"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Tramac</p>
  <div class="site-description" itemprop="description">Tramac写字的地方</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Tramac" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Tramac" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tramac</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
